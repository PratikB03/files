{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1daadf28",
   "metadata": {},
   "source": [
    "### Implement the continuous bag of words (CBOW) Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6611a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Data Preparation\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41efbbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text corpus\n",
    "corpus = [\"the quick brown fox jumps over the lazy dog\", \"the dog is in the garden\", \"the fox is in the forest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43b4ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and convert text to sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size including an additional index for padding\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "index_word = {v: k for k, v in tokenizer.word_index.items()}  # Reverse mapping for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d69f0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Generate Training Data\n",
    "window_size = 2  # Number of context words to consider on each side of the target word\n",
    "\n",
    "def generate_data(sequences, window_size):\n",
    "    X, y = [], []\n",
    "    for sequence in sequences:\n",
    "        for i, target in enumerate(sequence):\n",
    "            context = [sequence[j] for j in range(max(0, i - window_size), min(len(sequence), i + window_size + 1)) if j != i]\n",
    "            X.append(pad_sequences([context], maxlen=2 * window_size).flatten())  # Flatten to create a fixed input shape\n",
    "            y.append(target)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = generate_data(sequences, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afaf4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Train Model\n",
    "embedding_dim = 50  # Dimension of the word embeddings\n",
    "input_context = Input(shape=(2 * window_size,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding_layer\")(input_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "331777b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean embedding across context words\n",
    "average_embedding = Lambda(lambda x: tf.reduce_mean(x, axis=1))(embedding)\n",
    "output = Dense(vocab_size, activation='softmax')(average_embedding)\n",
    "\n",
    "cbow_model = Model(inputs=input_context, outputs=output)\n",
    "cbow_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60cc72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Output: Get Word Embeddings and Similarity\n",
    "# Retrieve embeddings by the explicitly named layer\n",
    "word_embeddings = cbow_model.get_layer('embedding_layer').get_weights()[0]\n",
    "\n",
    "# Function to find similar words based on cosine similarity\n",
    "def find_similar_words(word, embeddings, word_index, index_word, top_n=5):\n",
    "    word_vector = embeddings[word_index[word]].reshape(1, -1)\n",
    "    similarities = cosine_similarity(word_vector, embeddings).flatten()\n",
    "    similar_ids = similarities.argsort()[-top_n-1:-1][::-1]  # Get indices of most similar words\n",
    "    return [index_word[i] for i in similar_ids if i != word_index[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "399b8c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'fox': ['jumps', 'quick', 'forest', 'brown', 'is']\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Find words similar to \"fox\"\n",
    "print(\"Words similar to 'fox':\", find_similar_words(\"fox\", word_embeddings, tokenizer.word_index, index_word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
